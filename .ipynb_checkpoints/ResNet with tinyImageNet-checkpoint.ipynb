{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb541af4",
   "metadata": {},
   "source": [
    "The dataset Tiny ImageNet has to be in the 'data/' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f2234",
   "metadata": {},
   "source": [
    "Instruction to download and extract the data:\n",
    "\n",
    "wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "\n",
    "unzip tiny-imagenet-200.zip\n",
    "\n",
    "rm tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b203b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42be9f",
   "metadata": {},
   "source": [
    "# 0. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803c96a",
   "metadata": {},
   "source": [
    "### TinyImageNet does not have labels for the testing data. We will use the 10k validation images for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513cdce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pictures in folder n01443537\n",
      "500 pictures in folder n01629819\n",
      "500 pictures in folder n01641577\n",
      "498 pictures in folder n01644900\n",
      "496 pictures in folder n01698640\n",
      "499 pictures in folder n01742172\n",
      "492 pictures in folder n01768244\n",
      "497 pictures in folder n01770393\n",
      "497 pictures in folder n01774384\n",
      "495 pictures in folder n01774750\n",
      "500 pictures in folder n01784675\n",
      "498 pictures in folder n01855672\n",
      "499 pictures in folder n01882714\n",
      "499 pictures in folder n01910747\n",
      "499 pictures in folder n01917289\n",
      "495 pictures in folder n01944390\n",
      "497 pictures in folder n01945685\n",
      "500 pictures in folder n01950731\n",
      "500 pictures in folder n01983481\n",
      "500 pictures in folder n01984695\n",
      "499 pictures in folder n02002724\n",
      "500 pictures in folder n02056570\n",
      "498 pictures in folder n02058221\n",
      "498 pictures in folder n02074367\n",
      "497 pictures in folder n02085620\n",
      "497 pictures in folder n02094433\n",
      "494 pictures in folder n02099601\n",
      "495 pictures in folder n02099712\n",
      "492 pictures in folder n02106662\n",
      "491 pictures in folder n02113799\n",
      "492 pictures in folder n02123045\n",
      "492 pictures in folder n02123394\n",
      "492 pictures in folder n02124075\n",
      "497 pictures in folder n02125311\n",
      "493 pictures in folder n02129165\n",
      "500 pictures in folder n02132136\n",
      "500 pictures in folder n02165456\n",
      "497 pictures in folder n02190166\n",
      "497 pictures in folder n02206856\n",
      "500 pictures in folder n02226429\n",
      "497 pictures in folder n02231487\n",
      "495 pictures in folder n02233338\n",
      "499 pictures in folder n02236044\n",
      "499 pictures in folder n02268443\n",
      "500 pictures in folder n02279972\n",
      "500 pictures in folder n02281406\n",
      "500 pictures in folder n02321529\n",
      "496 pictures in folder n02364673\n",
      "495 pictures in folder n02395406\n",
      "493 pictures in folder n02403003\n",
      "494 pictures in folder n02410509\n",
      "499 pictures in folder n02415577\n",
      "496 pictures in folder n02423022\n",
      "498 pictures in folder n02437312\n",
      "491 pictures in folder n02480495\n",
      "485 pictures in folder n02481823\n",
      "495 pictures in folder n02486410\n",
      "496 pictures in folder n02504458\n",
      "499 pictures in folder n02509815\n",
      "492 pictures in folder n02666196\n",
      "488 pictures in folder n02669723\n",
      "497 pictures in folder n02699494\n",
      "500 pictures in folder n02730930\n",
      "498 pictures in folder n02769748\n",
      "465 pictures in folder n02788148\n",
      "453 pictures in folder n02791270\n",
      "481 pictures in folder n02793495\n",
      "484 pictures in folder n02795169\n",
      "497 pictures in folder n02802426\n",
      "491 pictures in folder n02808440\n",
      "491 pictures in folder n02814533\n",
      "485 pictures in folder n02814860\n",
      "494 pictures in folder n02815834\n",
      "495 pictures in folder n02823428\n",
      "474 pictures in folder n02837789\n",
      "471 pictures in folder n02841315\n",
      "486 pictures in folder n02843684\n",
      "460 pictures in folder n02883205\n",
      "492 pictures in folder n02892201\n",
      "491 pictures in folder n02906734\n",
      "488 pictures in folder n02909870\n",
      "492 pictures in folder n02917067\n",
      "497 pictures in folder n02927161\n",
      "497 pictures in folder n02948072\n",
      "479 pictures in folder n02950826\n",
      "500 pictures in folder n02963159\n",
      "493 pictures in folder n02977058\n",
      "494 pictures in folder n02988304\n",
      "472 pictures in folder n02999410\n",
      "497 pictures in folder n03014705\n",
      "499 pictures in folder n03026506\n",
      "494 pictures in folder n03042490\n",
      "466 pictures in folder n03085013\n",
      "499 pictures in folder n03089624\n",
      "498 pictures in folder n03100240\n",
      "480 pictures in folder n03126707\n",
      "483 pictures in folder n03160309\n",
      "493 pictures in folder n03179701\n",
      "500 pictures in folder n03201208\n",
      "474 pictures in folder n03250847\n",
      "479 pictures in folder n03255030\n",
      "496 pictures in folder n03355925\n",
      "466 pictures in folder n03388043\n",
      "499 pictures in folder n03393912\n",
      "495 pictures in folder n03400231\n",
      "480 pictures in folder n03404251\n",
      "480 pictures in folder n03424325\n",
      "496 pictures in folder n03444034\n",
      "480 pictures in folder n03447447\n",
      "452 pictures in folder n03544143\n",
      "489 pictures in folder n03584254\n",
      "481 pictures in folder n03599486\n",
      "497 pictures in folder n03617480\n",
      "484 pictures in folder n03637318\n",
      "495 pictures in folder n03649909\n",
      "500 pictures in folder n03662601\n",
      "492 pictures in folder n03670208\n",
      "496 pictures in folder n03706229\n",
      "491 pictures in folder n03733131\n",
      "454 pictures in folder n03763968\n",
      "495 pictures in folder n03770439\n",
      "498 pictures in folder n03796401\n",
      "468 pictures in folder n03804744\n",
      "491 pictures in folder n03814639\n",
      "487 pictures in folder n03837869\n",
      "463 pictures in folder n03838899\n",
      "494 pictures in folder n03854065\n",
      "473 pictures in folder n03891332\n",
      "477 pictures in folder n03902125\n",
      "467 pictures in folder n03930313\n",
      "488 pictures in folder n03937543\n",
      "490 pictures in folder n03970156\n",
      "485 pictures in folder n03976657\n",
      "496 pictures in folder n03977966\n",
      "500 pictures in folder n03980874\n",
      "493 pictures in folder n03983396\n",
      "474 pictures in folder n03992509\n",
      "485 pictures in folder n04008634\n",
      "480 pictures in folder n04023962\n",
      "481 pictures in folder n04067472\n",
      "495 pictures in folder n04070727\n",
      "479 pictures in folder n04074963\n",
      "485 pictures in folder n04099969\n",
      "499 pictures in folder n04118538\n",
      "495 pictures in folder n04133789\n",
      "498 pictures in folder n04146614\n",
      "497 pictures in folder n04149813\n",
      "474 pictures in folder n04179913\n",
      "498 pictures in folder n04251144\n",
      "497 pictures in folder n04254777\n",
      "492 pictures in folder n04259630\n",
      "490 pictures in folder n04265275\n",
      "484 pictures in folder n04275548\n",
      "495 pictures in folder n04285008\n",
      "473 pictures in folder n04311004\n",
      "482 pictures in folder n04328186\n",
      "465 pictures in folder n04356056\n",
      "473 pictures in folder n04366367\n",
      "493 pictures in folder n04371430\n",
      "471 pictures in folder n04376876\n",
      "497 pictures in folder n04398044\n",
      "485 pictures in folder n04399382\n",
      "488 pictures in folder n04417672\n",
      "498 pictures in folder n04456115\n",
      "480 pictures in folder n04465501\n",
      "491 pictures in folder n04486054\n",
      "494 pictures in folder n04487081\n",
      "473 pictures in folder n04501370\n",
      "477 pictures in folder n04507155\n",
      "500 pictures in folder n04532106\n",
      "473 pictures in folder n04532670\n",
      "494 pictures in folder n04540053\n",
      "492 pictures in folder n04560804\n",
      "473 pictures in folder n04562935\n",
      "494 pictures in folder n04596742\n",
      "500 pictures in folder n04597913\n",
      "497 pictures in folder n06596364\n",
      "500 pictures in folder n07579787\n",
      "500 pictures in folder n07583066\n",
      "500 pictures in folder n07614500\n",
      "498 pictures in folder n07615774\n",
      "500 pictures in folder n07695742\n",
      "500 pictures in folder n07711569\n",
      "497 pictures in folder n07715103\n",
      "499 pictures in folder n07720875\n",
      "500 pictures in folder n07734744\n",
      "500 pictures in folder n07747607\n",
      "499 pictures in folder n07749582\n",
      "493 pictures in folder n07753592\n",
      "499 pictures in folder n07768694\n",
      "500 pictures in folder n07871810\n",
      "500 pictures in folder n07873807\n",
      "500 pictures in folder n07875152\n",
      "498 pictures in folder n07920052\n",
      "494 pictures in folder n09193705\n",
      "496 pictures in folder n09246464\n",
      "500 pictures in folder n09256479\n",
      "499 pictures in folder n09332890\n",
      "494 pictures in folder n09428293\n",
      "496 pictures in folder n12267677\n"
     ]
    }
   ],
   "source": [
    "#Generate dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "###### TRAINING DATA #######\n",
    "#Load Training images and labels\n",
    "train_directory = \"./data/tiny-imagenet-200/train\" \n",
    "image_list=[]\n",
    "label_list=[]\n",
    "\n",
    "label_dic={} #convert label str to int from 0 \n",
    "\n",
    "\n",
    "\n",
    "for l,sub_dir in enumerate(os.listdir(train_directory)):\n",
    "    if not sub_dir in label_dic:\n",
    "        label_dic[sub_dir]=l\n",
    "    sub_dir_name=os.path.join(train_directory,sub_dir,'images')\n",
    "    n=0\n",
    "    for file in os.listdir(sub_dir_name):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".JPEG\")  or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img=np.array(Image.open(os.path.join(sub_dir_name,file)))\n",
    "            if(img.shape == (64, 64, 3)): #removing images without 3 channels\n",
    "                image_list.append(img)\n",
    "                label_list.append(int(label_dic[sub_dir]))\n",
    "                n+=1\n",
    "    print(n,'pictures in folder',sub_dir,sep=' ' )\n",
    "            \n",
    "X_tot=np.array(image_list)\n",
    "y_tot=np.array(label_list)\n",
    "\n",
    "###### TEST DATA #######\n",
    "\n",
    "#getting the labels from the txt file\n",
    "df = pd.read_table('./data/tiny-imagenet-200/val/val_annotations.txt', header=None)\n",
    "test_labels={} \n",
    "for index, row in df.iterrows():\n",
    "    test_labels[str(row[0])]= row[1]\n",
    "\n",
    "\n",
    "test_directory = \"./data/tiny-imagenet-200/val/images\" \n",
    "\n",
    "test_image_list=[]\n",
    "test_label_list=[]\n",
    "\n",
    "for file in os.listdir(test_directory):\n",
    "    filename = os.fsdecode(file)\n",
    "  \n",
    "    if filename.endswith(\".JPEG\")  or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img=np.array(Image.open(os.path.join(test_directory,file)))\n",
    "        if(img.shape == (64, 64, 3)): #removing images without 3 channels\n",
    "            test_image_list.append(img)\n",
    "            test_label_list.append(label_dic[test_labels[filename]])\n",
    "            \n",
    "X_test=np.array(test_image_list)\n",
    "y_test=tf.keras.utils.to_categorical(np.array(test_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df1e044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data:\n",
      "(9832, 64, 64, 3)\n",
      "(9832, 200)\n",
      "train data:\n",
      "(93179, 64, 64, 3)\n",
      "(93179, 200)\n",
      "val data:\n",
      "(5000, 64, 64, 3)\n",
      "(5000, 200)\n"
     ]
    }
   ],
   "source": [
    "shuffler=np.random.RandomState(seed=10).permutation(len(X_tot))\n",
    "X_tot = X_tot[shuffler]\n",
    "y_tot = y_tot[shuffler]\n",
    "y_tot=tf.keras.utils.to_categorical(y_tot)\n",
    "\n",
    "X_train = X_tot[5000:]\n",
    "y_train =  y_tot[5000:]\n",
    "X_val = X_tot[:5000]\n",
    "y_val =  y_tot[:5000]\n",
    "\n",
    "\n",
    "print('test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print('train data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('val data:')\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07260181",
   "metadata": {},
   "source": [
    "#### Saving the variables allows faster loading in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705823b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for future use without having to reload from the images folders\n",
    "np.save('data/X_test.npy', X_test)\n",
    "np.save('data/y_test.npy', y_test)\n",
    "np.save('data/X_train.npy', X_train)\n",
    "np.save('data/y_train.npy', y_train)\n",
    "np.save('data/X_val.npy', X_val)\n",
    "np.save('data/y_val.npy', y_val) \n",
    "\n",
    "X_test = np.load('data/X_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "X_train = np.load('data/X_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "X_val = np.load('data/X_val.npy')\n",
    "y_val = np.load('data/y_val.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4b28c",
   "metadata": {},
   "source": [
    "# 1. Using ResNet18 with ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae3a39",
   "metadata": {},
   "source": [
    "### We use our custom resnet model generator to get any resnet \n",
    "\n",
    "Imagenet has 200 different labels and is downsampled from ImageNet to a resolution of 64x64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "375a049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_resnet18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (ReLU)               (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 16, 16, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   36928       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (ReLU)      (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 64)   36928       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_relu (ReLU)        (None, 16, 16, 64)   0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (ReLU)      (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 64)   0           conv2_block1_relu[0][0]          \n",
      "                                                                 conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_relu (ReLU)        (None, 16, 16, 64)   0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    73856       conv2_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (ReLU)      (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 128)    73856       conv2_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 128)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_relu (ReLU)        (None, 8, 8, 128)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (ReLU)      (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 128)    0           conv3_block1_relu[0][0]          \n",
      "                                                                 conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_relu (ReLU)        (None, 8, 8, 128)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    295168      conv3_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (ReLU)      (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 256)    295168      conv3_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 256)    0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_relu (ReLU)        (None, 4, 4, 256)    0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (ReLU)      (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 256)    0           conv4_block1_relu[0][0]          \n",
      "                                                                 conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_relu (ReLU)        (None, 4, 4, 256)    0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    1180160     conv4_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (ReLU)      (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 512)    1180160     conv4_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_relu (ReLU)        (None, 2, 2, 512)    0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (ReLU)      (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 512)    0           conv5_block1_relu[0][0]          \n",
      "                                                                 conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_relu (ReLU)        (None, 2, 2, 512)    0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_out (GlobalAveragePooling2 (None, 512)          0           conv5_block2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 200)          102600      pool_out[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,706,952\n",
      "Trainable params: 12,697,224\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.custom_resnet import custom_resnet18\n",
    "\n",
    "resnet_18=custom_resnet18(input_shape=(64,64,3),n_classes=200)\n",
    "resnet_18.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70695a",
   "metadata": {},
   "source": [
    "## 1.1 Resnet without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 43s 57ms/step - loss: 5.9526 - acc: 0.0060 - val_loss: 5.2633 - val_acc: 0.0080\n",
      "Epoch 2/45\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 41s 57ms/step - loss: 5.1962 - acc: 0.0094 - val_loss: 5.4736 - val_acc: 0.0122\n",
      "Epoch 3/45\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 41s 57ms/step - loss: 4.9285 - acc: 0.0249 - val_loss: 4.9437 - val_acc: 0.0246\n",
      "Epoch 4/45\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 4.5929 - acc: 0.0509 - val_loss: 4.6150 - val_acc: 0.0562\n",
      "Epoch 5/45\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 41s 57ms/step - loss: 4.2738 - acc: 0.0823 - val_loss: 4.5985 - val_acc: 0.0688\n",
      "Epoch 6/45\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 3.9496 - acc: 0.1316 - val_loss: 4.2292 - val_acc: 0.1098\n",
      "Epoch 7/45\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 3.6861 - acc: 0.1735 - val_loss: 3.9184 - val_acc: 0.1476\n",
      "Epoch 8/45\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 3.4756 - acc: 0.2102 - val_loss: 3.6397 - val_acc: 0.1996\n",
      "Epoch 9/45\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 3.2395 - acc: 0.2496 - val_loss: 4.6642 - val_acc: 0.1368\n",
      "Epoch 10/45\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 3.0242 - acc: 0.2933 - val_loss: 3.6750 - val_acc: 0.2408\n",
      "Epoch 11/45\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 2.8166 - acc: 0.3305 - val_loss: 4.2122 - val_acc: 0.1842\n",
      "Epoch 12/45\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 2.6103 - acc: 0.3669 - val_loss: 3.5958 - val_acc: 0.2408\n",
      "Epoch 13/45\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 2.3994 - acc: 0.4118 - val_loss: 3.5374 - val_acc: 0.2568\n",
      "Epoch 14/45\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 2.1565 - acc: 0.4586 - val_loss: 3.7574 - val_acc: 0.2192\n",
      "Epoch 15/45\n",
      "new learning rate:  0.6000000238418579\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 1.5280 - acc: 0.5989 - val_loss: 3.4030 - val_acc: 0.3174\n",
      "Epoch 16/45\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.9950 - acc: 0.7328 - val_loss: 3.9054 - val_acc: 0.2948\n",
      "Epoch 17/45\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.7402 - acc: 0.8001 - val_loss: 4.2298 - val_acc: 0.2944\n",
      "Epoch 18/45\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.5119 - acc: 0.8643 - val_loss: 4.5729 - val_acc: 0.2958\n",
      "Epoch 19/45\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.3095 - acc: 0.9247 - val_loss: 5.1192 - val_acc: 0.2876\n",
      "Epoch 20/45\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.1773 - acc: 0.9620 - val_loss: 5.6795 - val_acc: 0.2820\n",
      "Epoch 21/45\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 58ms/step - loss: 0.0973 - acc: 0.9832 - val_loss: 6.0821 - val_acc: 0.2788\n",
      "Epoch 22/45\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - 42s 57ms/step - loss: 0.0568 - acc: 0.9916 - val_loss: 6.4850 - val_acc: 0.2812\n",
      "Epoch 23/45\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "727/727 [==============================] - ETA: 0s - loss: 0.0371 - acc: 0.9954"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "base_learning_rate=0.6\n",
    "epochs=45\n",
    "batch_size=128\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # no augmentation\n",
    "        )\n",
    "\n",
    "# CHange learning rate\n",
    "def change_learning_rate(epoch, lr):\n",
    "    if epoch % 14 == 0 and epoch:\n",
    "        print('new learning rate: ',lr)\n",
    "        return 0.1 *lr\n",
    "    return lr\n",
    "\n",
    "lr_callback = [LearningRateScheduler(change_learning_rate, verbose=1)]\n",
    "\n",
    "#Early stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=5)\n",
    "\n",
    "resnet_18.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9),loss='categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_resnet_18=resnet_18.fit(train_generator,\n",
    "                   steps_per_epoch = step_size_train,\n",
    "                   epochs = epochs,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                   callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8088d",
   "metadata": {},
   "source": [
    "#### How to save the model and reload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f067d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, acc = resnet_18.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "# Save the weights\n",
    "\n",
    "resnet_18.save_weights('./models/tinyImageNet/resnet_18')\n",
    "\n",
    "# reload from saved weights:\n",
    "model = custom_resnet18(input_shape=(64,64,3),n_classes=200)\n",
    "model.load_weights('./models/tinyImageNet/resnet_18')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c9712",
   "metadata": {},
   "source": [
    "Accuracy and loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_resnet_18.history['accuracy']\n",
    "val_acc = history_resnet_18.history['val_accuracy']\n",
    "\n",
    "loss = history_resnet_18.history['loss']\n",
    "val_loss = history_resnet_18.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy', color=\"blue\")\n",
    "plt.plot(val_acc, label='Validation Accuracy', color=\"r\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss', color=\"blue\")\n",
    "plt.plot(val_loss, label='Validation Loss', color=\"r\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59ca90",
   "metadata": {},
   "source": [
    "## 1.2 Resnet18 With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18_aug=custom_resnet18(input_shape=(64,64,3),n_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate=0.6\n",
    "epochs=45\n",
    "batch_size=128\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=8, #+-8 pixel shift max\n",
    "    height_shift_range=8,#+-8 pixel shift max\n",
    "    brightness_range=[0.8,1.2],\n",
    "    rotation_range=2 #very little rotation\n",
    "        )\n",
    "\n",
    "lr_callback = [LearningRateScheduler(change_learning_rate, verbose=1)]\n",
    "\n",
    "resnet_18_aug.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9),loss='categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "\n",
    "history_resnet_18_aug=resnet_18_aug.fit(train_generator,\n",
    "                   steps_per_epoch = step_size_train,\n",
    "                   epochs = epochs,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                   callbacks=[lr_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
